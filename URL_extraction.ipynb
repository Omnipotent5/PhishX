{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFiles/2.online-valid.csv: ['521848.txt', 'https://www.southbankmosaics.com', '31', 'www.southbankmosaics.com', '24', '0', 'com', '100', '1', '0.5229071', '0.061933179', '3', '1.1', '0.1', '0.2', '0.3', '18', '0.581', '0.4', '0.5', '0.6', '0.7', '0.8', '1.2', '0.032']\n",
      "Columns in DataFiles/1.Benign_list_big_final.csv: ['http://1337x.to/torrent/1048648/American-Sniper-2014-MD-iTALiAN-DVDSCR-X264-BST-MT/']\n",
      "Extracting features from phishing URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 5000/5000 [08:08<00:00, 10.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from legitimate URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 5000/5000 [07:05<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset created and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing required packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import whois\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # For parallel processing\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Function to preprocess URLs\n",
    "def preprocess_url(url):\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'http://' + url  # Assume HTTP if no protocol is specified\n",
    "    return url\n",
    "\n",
    "# Function to extract domain from URL\n",
    "def getDomain(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        return domain\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# Function to check if IP address is present in URL\n",
    "def havingIP(url):\n",
    "    try:\n",
    "        if re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Function to check if '@' symbol is present in URL\n",
    "def haveAtSign(url):\n",
    "    return 1 if '@' in url else 0\n",
    "\n",
    "# Function to get the length of the URL\n",
    "def getLength(url):\n",
    "    return 1 if len(url) >= 54 else 0\n",
    "\n",
    "# Function to get the depth of the URL\n",
    "def getDepth(url):\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        return len([i for i in path.split('/') if i])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Function to check for redirection '//' in URL\n",
    "def redirection(url):\n",
    "    pos = url.rfind('//')\n",
    "    return 1 if pos > 6 else 0\n",
    "\n",
    "# Function to check if 'http' or 'https' is in the domain part of the URL\n",
    "def httpDomain(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    return 1 if 'http' in domain else 0\n",
    "\n",
    "# Function to check for URL shortening services\n",
    "def tinyURL(url):\n",
    "    shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|qr\\.ae|adf\\.ly|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|po\\.st|bc\\.vc|twit\\.this|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net\"\n",
    "    match = re.search(shortening_services, url)\n",
    "    return 1 if match else 0\n",
    "\n",
    "# Function to check for prefix or suffix '-' in domain\n",
    "def prefixSuffix(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    return 1 if '-' in domain else 0\n",
    "\n",
    "# Function to check for DNS record availability\n",
    "def dnsRecord(domain):\n",
    "    try:\n",
    "        whois.whois(domain)\n",
    "        return 0\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# Function to check web traffic ranking\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        alexa_rank = requests.get(f\"http://data.alexa.com/data?cli=10&url={url}\", timeout=3)\n",
    "        rank = int(re.search(r'<POPULARITY[^>]*TEXT=\"(\\d+)\"', alexa_rank.text).group(1))\n",
    "        return 1 if rank < 100000 else 0\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# Function to check domain age\n",
    "def domainAge(domain_name):\n",
    "    try:\n",
    "        creation_date = domain_name.creation_date\n",
    "        expiration_date = domain_name.expiration_date\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "        age = (expiration_date - creation_date).days / 30\n",
    "        return 1 if age >= 12 else 0\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# Function to check domain end period\n",
    "def domainEnd(domain_name):\n",
    "    try:\n",
    "        expiration_date = domain_name.expiration_date\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "        end = (expiration_date - pd.Timestamp.now()).days / 30\n",
    "        return 1 if end >= 6 else 0\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# Function to check for IFrame redirection\n",
    "def iframe(response):\n",
    "    if not response or response == \"\":\n",
    "        return 1\n",
    "    try:\n",
    "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except AttributeError:\n",
    "        return 1\n",
    "\n",
    "# Function to check for mouse over events\n",
    "def mouseOver(response):\n",
    "    if not response or response == \"\":\n",
    "        return 1\n",
    "    try:\n",
    "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "\n",
    "# Function to check for right-click disabling\n",
    "def rightClick(response):\n",
    "    if not response or response == \"\":\n",
    "        return 1\n",
    "    try:\n",
    "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except AttributeError:\n",
    "        return 1\n",
    "\n",
    "# Function to check for website forwarding\n",
    "def forwarding(response):\n",
    "    if not response or response == \"\":\n",
    "        return 1\n",
    "    try:\n",
    "        if len(response.history) <= 2:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except AttributeError:\n",
    "        return 1\n",
    "\n",
    "# Function to extract features from URLs\n",
    "def featureExtraction(url, label):\n",
    "    features = []\n",
    "    url = preprocess_url(url)\n",
    "\n",
    "    # Address bar features\n",
    "    features.append(getDomain(url))\n",
    "    features.append(havingIP(url))\n",
    "    features.append(haveAtSign(url))\n",
    "    features.append(getLength(url))\n",
    "    features.append(getDepth(url))\n",
    "    features.append(redirection(url))\n",
    "    features.append(httpDomain(url))\n",
    "    features.append(tinyURL(url))\n",
    "    features.append(prefixSuffix(url))\n",
    "\n",
    "    # Domain-based features\n",
    "    dns = 0\n",
    "    domain_name = None\n",
    "    try:\n",
    "        domain = urlparse(url).netloc\n",
    "        domain_name = whois.whois(domain)\n",
    "    except Exception as e:\n",
    "        dns = 1\n",
    "\n",
    "    features.append(dns)\n",
    "    features.append(web_traffic(url))\n",
    "    features.append(1 if dns == 1 else domainAge(domain_name))\n",
    "    features.append(1 if dns == 1 else domainEnd(domain_name))\n",
    "\n",
    "    # HTML & JS features\n",
    "    response = None\n",
    "    try:\n",
    "        response = requests.get(url, timeout=3)\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.Timeout):\n",
    "        response = None  # Skip if it takes too long\n",
    "\n",
    "    features.append(iframe(response))\n",
    "    features.append(mouseOver(response))\n",
    "    features.append(rightClick(response))\n",
    "    features.append(forwarding(response))\n",
    "    features.append(label)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to load dataset and identify URL column\n",
    "def load_dataset(file_path, url_column=None):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please ensure it is in the correct directory.\")\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    if data.empty:\n",
    "        raise ValueError(f\"The dataset {file_path} is empty. Please provide a valid dataset.\")\n",
    "\n",
    "    # Display columns for debugging\n",
    "    print(f\"Columns in {file_path}: {data.columns.tolist()}\")  \n",
    "\n",
    "    # Manually select the URL column if not provided\n",
    "    if not url_column:\n",
    "        url_column = data.columns[0]  # Default to the first column if none specified\n",
    "\n",
    "    if url_column not in data.columns:\n",
    "        raise KeyError(f\"The specified URL column '{url_column}' does not exist in the dataset {file_path}. Please verify the dataset structure.\")\n",
    "\n",
    "    return data, url_column\n",
    "\n",
    "# Load Phishing URLs Data\n",
    "phishing_data_path = 'DataFiles/2.online-valid.csv'\n",
    "phish_data, phish_url_column = load_dataset(phishing_data_path, url_column='https://www.southbankmosaics.com')\n",
    "\n",
    "# Load Legitimate URLs Data\n",
    "legitimate_data_path = 'DataFiles/1.Benign_list_big_final.csv'\n",
    "legit_data, legit_url_column = load_dataset(legitimate_data_path)\n",
    "\n",
    "# Sample 5,000 phishing and legitimate URLs\n",
    "phish_sample = phish_data.sample(n=5000, random_state=12).reset_index(drop=True)\n",
    "legit_sample = legit_data.sample(n=5000, random_state=12).reset_index(drop=True)\n",
    "\n",
    "# Function to extract features in parallel\n",
    "def parallel_feature_extraction(url_list, label):\n",
    "    features = []\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:  # Adjust max_workers as needed\n",
    "        futures = {executor.submit(featureExtraction, url, label): url for url in url_list}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting features\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                features.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL: {futures[future]} - {e}\")\n",
    "    return features\n",
    "\n",
    "# Extract features from phishing URLs in parallel\n",
    "print(\"Extracting features from phishing URLs...\")\n",
    "phish_features = parallel_feature_extraction(phish_sample[phish_url_column], label=1)\n",
    "\n",
    "# Extract features from legitimate URLs in parallel\n",
    "print(\"Extracting features from legitimate URLs...\")\n",
    "legit_features = parallel_feature_extraction(legit_sample[legit_url_column], label=0)\n",
    "\n",
    "# Combine phishing and legitimate features into one dataset\n",
    "final_dataset = phish_features + legit_features\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "columns = ['Domain', 'Having_IP', 'Have_At_Sign', 'URL_Length', 'URL_Depth', 'Redirection', \n",
    "           'HTTP_in_Domain', 'TinyURL', 'Prefix_Suffix', 'DNS_Record', 'Web_Traffic', \n",
    "           'Domain_Age', 'Domain_End', 'Iframe', 'Mouse_Over', 'Right_Click', 'Web_Forwards', 'Label']\n",
    "\n",
    "final_df = pd.DataFrame(final_dataset, columns=columns)\n",
    "final_df.to_csv('final_dataset.csv', index=False)\n",
    "\n",
    "print(\"Final dataset created and saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
